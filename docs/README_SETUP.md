# S3 ACID Database - Complete Setup Guide

## âœ… Fully Automated Setup

Everything is configured to work out-of-the-box with **zero manual setup required**!

## Quick Start (One Command)

```bash
docker compose up -d
```

Wait ~20 seconds for services to initialize, then you're ready to go!

### What Happens Automatically:

1. **MinIO** starts (S3-compatible storage)
2. **minio-init** container automatically:
   - Creates `test-bucket` bucket
   - Creates `iceberg-warehouse` bucket
   - Sets public access policies
   - Exits when done âœ“
3. **Iceberg REST Catalog** starts
4. **Lambda API** starts with full Iceberg support

## Verify Setup

```bash
curl -X POST http://localhost:8080/2015-03-31/functions/function/invocations \
  -H "Content-Type: application/json" \
  -d '{"httpMethod":"GET","path":"/health"}' | jq .
```

Expected output:
```json
{
  "statusCode": 200,
  "body": "{\"status\": \"healthy\", \"service\": \"S3 ACID Database\", \"version\": \"1.0.0\"}"
}
```

## Using Postman

### Import Collection
1. Open Postman
2. Import â†’ Upload Files
3. Select: `S3_ACID_Database.postman_collection.json`

### Run Requests in Order
The collection contains 11 ready-to-use requests:

1. **Health Check** - Verify API is running âœ“
2. **CREATE TABLE** - Create users table with schema
3. **WRITE** - Insert 3 users (Alice, Bob, Charlie)
4. **QUERY - Get all users** - Retrieve all records
5. **QUERY - Filter by name** - Find Alice specifically
6. **QUERY - Filter by age** - Find users where age > 28
7. **QUERY - With sorting** - Sort by age descending
8. **UPDATE** - Change Alice's age from 30 to 31
9. **DELETE** - Soft delete Charlie (_deleted=true)
10. **LIST TABLES** - Show all tables in namespace
11. **DESCRIBE TABLE** - Get schema and statistics

**Important:** Wait 1-2 seconds between CREATE_TABLE and WRITE for table metadata to sync.

## Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client    â”‚
â”‚  (Postman)  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Lambda Container   â”‚
â”‚  (Port 8080)        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ PyIceberg (Write) â”‚
â”‚ â€¢ DuckDB (Read)     â”‚
â”‚ â€¢ Polars (Convert)  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚      â”‚
       v      v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MinIO   â”‚ â”‚ REST Catalog â”‚
â”‚ (S3 API) â”‚ â”‚  (Metadata)  â”‚
â”‚ Port 9005â”‚ â”‚   Port 8181  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Write Path
```
Client â†’ Lambda â†’ PyIceberg â†’ REST Catalog (register metadata)
                            â†’ MinIO (write Parquet + metadata)
```

### Read Path
```
Client â†’ Lambda â†’ PyIceberg (get metadata location)
               â†’ DuckDB iceberg_scan(metadata) â†’ MinIO (read Parquet)
```

## Filter Syntax

All filters use this format:
```json
{
  "filter": {
    "field_name": {"operator": "value"}
  }
}
```

### Examples

**Equal:**
```json
{"name": {"eq": "Alice"}}
```

**Greater than:**
```json
{"age": {"gt": 25}}
```

**Range:**
```json
{"age": {"between": [25, 35]}}
```

**Pattern matching:**
```json
{"email": {"like": "%example.com"}}
```

**Multiple conditions (AND):**
```json
{
  "and": [
    {"age": {"gte": 25}},
    {"name": {"like": "A%"}}
  ]
}
```

**Multiple conditions (OR):**
```json
{
  "or": [
    {"age": {"lt": 30}},
    {"age": {"gt": 40}}
  ]
}
```

## Services & Ports

| Service | Port | URL | Credentials |
|---------|------|-----|-------------|
| Lambda API | 8080 | http://localhost:8080 | - |
| MinIO Console | 9006 | http://localhost:9006 | minioadmin / minioadmin |
| MinIO API | 9005 | http://localhost:9005 | - |
| REST Catalog | 8181 | http://localhost:8181 | - |

## Viewing Data

### MinIO Console
1. Open: http://localhost:9006
2. Login: `minioadmin` / `minioadmin`
3. Navigate to: `test-bucket` â†’ `iceberg-warehouse` â†’ `test_tenant_default` â†’ `users`

You'll see:
- `data/` - Parquet files with actual records
- `metadata/` - Iceberg metadata (.json, .avro files)

### Docker Logs
```bash
# View Lambda logs
docker compose logs -f lambda-api

# View MinIO logs
docker compose logs -f minio

# View REST Catalog logs
docker compose logs -f iceberg-rest

# View init container logs (buckets creation)
docker compose logs minio-init
```

## System Fields

Every record automatically includes:

| Field | Type | Description |
|-------|------|-------------|
| `_tenant_id` | string | Multi-tenant isolation |
| `_record_id` | string | Unique MD5 hash of record |
| `_timestamp` | timestamp | Creation/update time |
| `_version` | int | Optimistic locking version |
| `_deleted` | boolean | Soft delete flag |
| `_deleted_at` | timestamp | Deletion time (null if not deleted) |

Queries automatically exclude deleted records (`WHERE _deleted IS NOT TRUE`).

## Common Operations

### Fresh Start
```bash
docker compose down -v  # Remove volumes
docker compose up -d    # Start fresh
```

### View Status
```bash
docker compose ps
```

### Stop Services
```bash
docker compose down
```

### Restart Lambda (if crashed)
```bash
docker compose restart lambda-api
```

## Troubleshooting

### Lambda doesn't respond
**Solution:** Wait 15-20 seconds after `docker compose up -d` for Lambda to initialize.

### "Empty reply from server"
**Solution:** Lambda Runtime Emulator initializes on first request. Wait 2 seconds and retry.

### CREATE TABLE works but WRITE fails
**Solution:** Wait 1-2 seconds between CREATE_TABLE and WRITE for metadata sync.

### Query returns 0 results
**Checklist:**
- âœ“ Did WRITE return `records_written: N`?
- âœ“ Is `tenant_id` correct in query?
- âœ“ Is filter syntax correct? (e.g., `{"name": {"eq": "value"}}`)

### MinIO Console shows no files
**Solution:**
```bash
docker compose logs minio-init  # Verify buckets created
```

## Production Deployment

### Switch to AWS Glue Catalog

1. Update `docker-compose.yml`:
```yaml
environment:
  CATALOG_TYPE: glue
  AWS_ACCOUNT_ID: your-account-id
```

2. Update `src/operations_full_iceberg.py`:
```python
from pyiceberg.catalog.glue import GlueCatalog

catalog = GlueCatalog(
    name="glue",
    **{
        "region_name": AWS_REGION,
        "s3.region": AWS_REGION,
        "warehouse": f"s3://{BUCKET_NAME}/iceberg-warehouse"
    }
)
```

3. Replace MinIO with real S3:
```yaml
environment:
  BUCKET_NAME: your-real-bucket
  # Remove S3_ENDPOINT (use real AWS S3)
```

## Testing

Run automated test suite:
```bash
chmod +x test_workflow.sh
./test_workflow.sh
```

This validates:
- âœ“ CREATE TABLE
- âœ“ WRITE (3 records)
- âœ“ QUERY (all records)
- âœ“ QUERY (with filter)
- âœ“ UPDATE (modify record)
- âœ“ DELETE (soft delete)
- âœ“ LIST TABLES
- âœ“ DESCRIBE TABLE

## File Structure

```
s3-acid-database/
â”œâ”€â”€ docker-compose.yml                          # Service orchestration
â”œâ”€â”€ Dockerfile                                   # Lambda container
â”œâ”€â”€ S3_ACID_Database.postman_collection.json    # âœ“ Ready-to-use API tests
â”œâ”€â”€ QUICKSTART.md                                # Quick reference
â”œâ”€â”€ README_SETUP.md                              # This file
â”œâ”€â”€ test_workflow.sh                             # Automated tests
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ lambda_handler.py                        # API Gateway handler
â”‚   â”œâ”€â”€ operations_full_iceberg.py               # âœ“ PyIceberg + DuckDB
â”‚   â”œâ”€â”€ models.py                                # Type-safe Pydantic models
â”‚   â”œâ”€â”€ query_builder.py                         # SQL query builder
â”‚   â””â”€â”€ config.py                                # Configuration
â””â”€â”€ pyproject.toml                               # Dependencies
```

## Dependencies

Automatically installed in Docker container:
- **pyiceberg** - Iceberg table operations
- **duckdb** - Query engine with iceberg_scan
- **polars** - Data transformation
- **pyarrow** - Columnar data format
- **pydantic** - Type validation
- **boto3** - S3 operations

## Next Steps

1. âœ“ `docker compose up -d`
2. âœ“ Import Postman collection
3. âœ“ Run "Health Check" request
4. âœ“ Run "CREATE TABLE" request
5. âœ“ Run "WRITE" request
6. âœ“ Run "QUERY" requests
7. âœ“ Experiment with filters and updates
8. âœ“ View data in MinIO Console

**Everything is ready to use! No manual setup required.**

## Support

For issues:
1. Check `docker compose logs lambda-api`
2. Verify `docker compose ps` shows all services as healthy
3. Ensure ports 8080, 9005, 9006, 8181 are available

**The system is fully automated and production-ready!** ğŸš€
